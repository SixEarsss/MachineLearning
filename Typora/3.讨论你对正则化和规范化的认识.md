# 讨论你对正则化和规范化的认识

### 正则化：

##### 目的：

通过向模型的损失函数中添加惩罚项来约束模型的复杂性，以防止模型过拟合。平衡模型的拟合能力和其泛化能力。

##### 1.L1正则化（Lasso正则化）：

L1 正则化公式为：

$$
\text{Loss}_{\text{L1}} = \text{Loss}_{\text{original}} + \lambda \sum_{i=1}^{n} |w_i|
$$
这里的L1指的是L1范数，即向量中所有元素的绝对值之和。 

其中，正则化参数 $\lambda$ 是控制正则化强度的参数，而权重参数 $w_i$ 是模型的权重。

L1 范数定义为：
$$
\|x\|_1 = \sum_{i=1}^{n} |x_i|
$$
L1正则化通过在损失函数中添加权重绝对值之和的惩罚项来控制模型的复杂性。

##### 特点：

1.特征选择：L1正则化可以使某些权重变为零，从而实现特征选择。这对于高维数据集尤其有效，因为它能够有效地去除不相关的特征。

2.稀疏化：L1 正则化导致模型稀疏，即只有少数特征在最终模型中具有非零权重。

3.适用于特征较多，且希望进行特征选择的情况。

4.优化过程复杂。

##### 2.L2正则化：

L2 正则化公式为：

$$
\text{Loss}_{\text{L2}} = \text{Loss}_{\text{original}} + \lambda \sum_{i=1}^{n} w_i^2
$$
这里的L2指的是L2范数，即向量中所有元素平方和的平方根。

其中，正则化参数 $\lambda$ 是控制正则化强度的参数，而权重参数 $w_i$ 是模型的权重。

L2范数的定义为：

L2 范数定义为：
$$
\|x\|_2 = \sqrt{\sum_{i=1}^{n} x_i^2}
$$
L2 正则化通过在损失函数中添加权重平方和的惩罚项来控制模型的复杂性。

##### 特点：

1.L2正则化对于绝对值较大的权重予以很重的惩罚，对于绝对值很小的权重予以非常非常小的惩罚，当权重绝对值趋近于0时，基本不惩罚。这个性质与L2的平方项有关系，即越大的数，其平方越大，越小的数，比如小于1的数，其平方反而越小。

2.平滑性：L2 正则化导致的惩罚项是光滑的，因此优化过程相对简单，常用的优化算法如梯度下降法可以直接应用（这个函数权重的平方和是一个光滑函数，连续且可导）。

3.权重衰减：L2 正则化会将所有的权重缩小，但不会将它们变为零，适用于特征间有相关性的情况。

4.提高模型的稳定性：通过缩小权重，L2 正则化有助于提高模型的稳定性和泛化能力。

5.适用于特征间相关性较高的情况。

6.优化过程简单。

##### 3.弹性网：

弹性网正则化结合了 L1 和 L2 正则化的特点，其损失函数为： 

​							$$ \text{Loss}_{\text{Elastic Net}} = \text{Loss}_{\text{original}} + \lambda_1 \sum_{i=1}^{n} |w_i| + \lambda_2 \sum_{i=1}^{n} w_i^2 $$

适用于特征较多且相关性较高的情况，能够实现特征选择，同时保持模型的平滑性。

##### 4.超参数选择：

1.λ 是关键超参数，需要通过交叉验证等方法进行选择。较大的λ会增加正则化效果，减少过拟合，但也可能导致欠拟合。

2.在弹性网中，λ1和λ2的比例需要根据数据的特征进行调节。

### 规范化：

##### 目的：

加速模型训练，稳定梯度以及防止过拟合。

基本原理是通过对神经网络中的激活值（输出）进行标准化，使每一层的输出保持在一个稳定的范围内，防止输出值过大或过小，从而改善梯度传播的稳定性，并最终加快模型的收敛速度。

##### 1.Batch Normalization（批规范化）：

###### 背景及一些概念补充：

是一种用来解决深度神经网络中训练不稳定和收敛慢的技术。它的核心原理是在网络的每一层中，将激活值进行归一化（标准化），使得输出的均值为 0，方差为 1，然后再通过可学习的参数进行缩放和平移，以保留模型的表达能力。

深度神经网络通常会随着层数的增加，出现内部协变量转移的问题。这意味着每一层的输入分布会在训练过程中不断变化，导致训练变得缓慢，并且模型对初始权重设置比较敏感。Batch Normalization 通过规范化每一批次的数据来解决这一问题。

**内部协变量转移**  是指在训练神经网络时，网络的每一层的输入分布会随着参数更新而不断变化的现象。当神经网络训练时，前几层的参数会不断更新，这会导致后续层的输入分布发生变化。简单来说，某一层的输入值是前一层的输出值，随着前一层权重和偏置的更新，输入分布也会跟着变化。如果每一层的输入分布不断变化，网络就需要不断适应这些变化的输入，这增加了模型训练的难度，使得模型需要更长时间才能学习到稳定的特征表示。

###### 工作机制：

1.计算mini-batch的均值和方差

（在使用 **mini-batch** 时，神经网络的参数更新是基于这个批次中的所有样本来进行的。每个 mini-batch 包含若干个样本，模型会用这些样本的 **梯度平均值** 来更新参数。）

均值：
$$
\mu_B = \frac{1}{m} \sum_{i=1}^{m} x_i
$$
这里的m是mini-batch的大小。

方差：
$$
\sigma_B^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2
$$
2.规范化：

标准化公式：
$$
\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
$$
这里，将每个样本的激活值 $x_i$减去 mini-batch 的均值 $u_B$ ，并除以标准差$\sqrt{\sigma_B^2 + \epsilon}$ (方差的平方根加上一个小常数$\epsilon $用以防止分母为零）。

这一步将激活值标准化为均值为 0、方差为 1 的分布，使得模型在训练时各层输入的分布相对稳定，减轻了内部协变量转移的问题。

3.缩放和平移：

最终输出：
$$
y_i = \gamma \hat{x}_i + \beta
$$
这里，$\gamma$是可学习的缩放参数，$\beta$是可学习的平移参数。标准化后的激活值 $\hat{x}_i$经过缩放和偏移后得到最终的输出 $y_i$。

通过引入 $\gamma$和 $\beta$，模型可以灵活地调整标准化后的激活值，使得它仍然可以表达复杂的特征。这种灵活性有助于保持模型的表达能力，同时提高训练的稳定性。

###### 作用：

1.减轻内部协变量转移

2.加快训练速度：通过规范化，网络各层的输入分布更加稳定，这让优化过程变得更容易。此外，Batch Normalization 允许使用更高的学习率，因为标准化后，梯度的变化变得更加平稳，从而减少了因过大梯度导致的震荡。

3.减少过拟合：由于它是在 mini-batch 上计算均值和方差，这种操作引入了一定的随机性，每个 mini-batch 的均值和方差会有所不同，这种噪声有助于防止模型过度拟合训练集。

4.增强模型的鲁棒性：鲁棒性意味着模型在面对不同的数据分布或噪声时，能够表现得更加稳定，并能正确地学习和泛化。Batch Normalization 中引入的 可学习参数 $\gamma$和 $\beta$，让模型即使在标准化后的状态下，也具备灵活的调整能力，适应不同的数据分布。

5.适用于深度网络：随着神经网络变得越来越深，每层的输出都会成为下一层的输入。没有 Batch Normalization 时，这些输入的分布可能会随着网络深度的增加变得越来越不稳定，导致训练困难。通过 Batch Normalization，我们能够确保每一层的输入数据保持在一个稳定的分布范围内，从而使得深度网络的训练变得更加有效。

6.允许更高的学习率：使用 Batch Normalization 后，网络的梯度变化更加平稳，模型的训练过程变得更加稳定，这允许我们使用更高的学习率。更高的学习率意味着参数的更新幅度更大，模型可以更快地从误差中恢复，进一步加速收敛。

##### 2.Layer Normalization（层规范化）：

###### 背景及一些概念补充：

在**Layer Normalization** 中，**“layer”** 指的是神经网络的**单层神经元**。**Layer Normalization** 是在 **每个样本的神经元层** 维度（特征维度）上进行操作。具体来说，它对一个样本在某一层的所有神经元（特征）计算均值和方差，然后对该样本的激活值进行标准化。

###### 工作机制：

1.均值μ：
$$
\mu = \frac{1}{n} \sum_{i=1}^{n} x_i
$$
其中，$n$是该层的神经元数量，$xi$是第 $i$个神经元的激活值。

2.方差$\sigma^2$：
$$
\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2
$$
方差是在该层所有神经元的激活值上计算的。

3.标准化：
$$
\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}
$$
4.缩放和平移：
$$
y_i = \gamma \hat{x}_i + \beta
$$
其中，$\gamma$是缩放参数，$\beta$是平移参数，都是可学习的参数。

###### 特点：

1.不依赖 mini-batch，适应小批量训练： Layer Normalization 的一个显著优势是，它不依赖 mini-batch 大小。在每个样本的特征维度上进行标准化，所以即使 batch size 很小甚至为 1，也可以正常工作。这对像 RNNs 等需要处理逐个样本（或小批次）的任务特别有效。

而当批次很小时（甚至 batch size 为 1）时，Batch Normalization 无法有效计算 mini-batch 内的均值和方差。

2.适用于序列模型：在 RNN 或 Transformer 等序列模型中，Layer Normalization 能很好地处理时间步长之间的依赖关系。因为它只在每个时间步的神经元层上进行标准化，而不是在整个 batch 维度上进行计算。这使得它更适合序列数据处理和自然语言处理任务。

###### 作用：

1.消除不同特征的数值差异对训练的影响：

对每个样本的所有特征进行标准化，确保不同特征的数值差异不会影响模型的训练效果。

这里我看过一个很好的例子，在一个房价预测模型中，房价数值可能比面积、房间数等特征大得多。这种差异会导致特征值范围较大的特征对神经网络参数的影响远大于其他特征，从而使模型难以收敛或偏向某些特征。而Layer Normalization通过对每个样本的所有特征进行标准化，将它们调整到相似的数值范围，使得特征之间的影响更加均衡，避免某些特征因为数值较大而主导模型的训练。

2.加快训练速度（处理协变量偏移）：

Layer Normalization 通过对每一层的输出进行标准化，保证每一层输出的分布更加稳定。标准化后的输出具有均值为 0、方差为 1 的分布，模型可以更稳定地更新权重，训练过程更加高效。

3.减少过拟合：

Layer Normalization 通过让每个样本的特征更均匀地参与模型训练，减少模型对某些特征的过度依赖，进而降低过拟合的风险。
