

# relu激活函数与sgimoid和tanh有哪些优势

### sgimoid函数：

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

###### 优点：

1.值域范围：输出范围是0到1之间，非常适合处理概率问题，如表示二分类的类别或者用于表示置信度。

2.平滑性：梯度平滑且该函数可微，使得在反向传播过程中容易计算梯度。

##### 缺点

1.梯度消失问题：在反向传播算法中，神经网络通过逐层计算梯度来更新权重。假设有多层网络，每层的梯度是前一层梯度与激活函数的导数相乘得到的。

sigmoid函数的导数都是小于0.25的，那么在进行反向传播的时候，梯度相乘结果会慢慢的趋向于0。最终会导致权重更新非常缓慢，神经网络的学习效果差，特别是在靠近输入层的网络中表现得更为明显。

2.容易陷入局部最优解：由于sigmoid函数的梯度较小，使得神经网络在训练时容易陷入局部最优解。

3.计算成本较大：sigmoid函数包含指数运算，导致计算量较大。

4.输出范围有限：函数的输出范围是0到1，不适合输出具有更广泛取值范围的任务。

### Tanh函数（双曲正切函数）：

$$
\text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

Tanh 函数的导数为：

$$
\frac{d}{dx} \text{tanh}(x) = 1 - \text{tanh}^2(x)
$$

### 优点

1.输出是零中心：Tanh函数的输出范围是（-1，1），且是零中心的。这有助于神经网络的权重更新时更加平衡，避免了sigmoid函数带来的偏差问题。

2.收敛速度较sigmoid函数更快：Tanh函数对于小输入值即靠近原点时斜率较高，这意味着对于小输入值，函数的导数较大，这有助于反向传播时权重更新更为有效。

### 缺点

1.梯度消失问题：Tanh函数虽然在输入值接近0时，梯度较大，有助于权重更新，但当输入值较大或较小时，输出值趋近于1或-1，这时梯度接近于0，这会导致梯度消失问题，即在反向传播时梯度逐渐变小，尤其是在深度神经网络中，随着层数的增加，前面层的梯度可能接近于0，从而导致模型训练变得非常缓慢，甚至无法有效更新参数。

2.饱和区带来的训练问题：Tanh在输入值较大或较小时会趋近于1或-1，这些区域就是所谓的饱和区，当输入进入饱和区后，Tanh函数的导数（梯度）变得非常小，会导致权重几乎得不到更新。

3.计算复杂度高：Tanh函数的计算也涉及到指数运算，这会增加计算复杂度。

4.输出较为稠密：Tanh函数的输出范围是（-1，1），这意味着Tanh函数的输出总是非零的（除非输入为0）。因此，Tanh函数的输出较为稠密，导致它的神经元几乎总在激活。

5.不适合用作输出层：对于分类问题，大多数情况下分类器需要一个标准化的概率输出如0到1之间的值，这种情况下Tanh函数没有sigmoid函数适用。

### ReLU函数

$$
\text{ReLU}(x) = \max(0, x)
$$

$$
\text{ReLU}'(x) =
\begin{cases} 
1, & \text{if } x > 0 \\
0, & \text{if } x \leq 0 
\end{cases}
$$

```
def relu(x):
    a = torch.zeros_like(x)
    return torch.max(x, a)

```



### 优势

##### 1.解决梯度消失问题和饱和问题：

在正区间，ReLU的导数始终为1，这避免了梯度消失问题。由于正输入的梯度不会衰减，ReLU能够保持较大的梯度，尤其是在深层神经网络中。

Sigmoid和Tanh都是双曲非线性函数，输出值会逐渐饱和，这种饱和效应降低了梯度，使得梯度更新不够有效。ReLU本质上是分段线性的，它引入了非线性，同时避免了sigmoid和tanh那种极端情况下的饱和问题。通过ReLU，神经网络依然能够保持很强的非线性表达能力，同时保留了较好的梯度流动。

##### 2.计算效率：

 ReLU只涉及简单的阈值判断，输出要么是0要么是输入本身，这使得ReLU的计算非常简单且快速。因此，在大规模神经网络中，ReLU显著提高了计算效率。

##### 3.稀疏性：

当输入为负数时，ReLU的输出会产生大量0，这意味着神经网络中的很多神经元在特定输入下不会被激活（输出为零），从而产生了稀疏激活，有效减少了计算量。

##### 4.训练速度：

ReLU在正区间的梯度始终为1，这意味着神经网络的权重更新较大，能够加速梯度下降，显著提升训练速度，此外，ReLU产生的稀疏激活能够减少神经元的参与，减少计算开销，从而进一步提高训练效率。

##### 5.解决过拟合问题：

由于ReLU函数的稀疏激活特性，ReLU能有效减少参与训练的神经元数量，泛化神经网络学习到的特征，帮助模型避免过拟合（因为很多神经元在当前计算中不参与激活或计算，减少了网络的复杂性，使其不容易去过度拟合训练数据中的噪声或异常）。

### 缺点

##### 死神经元问题：

当神经元的输入值长期为负数或 0 时，ReLU 函数的输出将始终为 0，这意味着在前向传播中，这些神经元的输出没有激活。

由于 ReLU 的导数在负数区间是 0，所以在反向传播过程中，这些神经元的梯度也为 0，导致权重不会更新。

随着训练的进行，如果某些神经元总是被激活为 0，反向传播时这些神经元的梯度为 0，它们的权重便无法进一步调整。这些神经元就会完全失去作用，成为“死神经元”。



