#  深度学习中Epoch、Batch和Iteration都是什么

### Epoch:

所有训练样本在神经网络中都进行了一次正向传播和一次反向传播，一个Epoch是指模型完成对整个训练数据集的一次完整遍历，也就是说一个epoch等于使用训练集中的全部样本训练一次。

深度学习模型通常需要多个Epoch来逐步优化权重，使得模型的性能逐渐提高，随着epoch的增加，模型会逐渐从数据中学到更多的特征。但如果Epoch过多，模型可能会过拟合。

### Batch：

Batch是模型一次处理的一小部分数据集样本。

由于数据集通常非常大，模型无法一次性将整个数据集放入内存中进行训练。因此，数据集被分成多个小批次（Batch），每个批次包含一定数量的样本。

**Batch size（批量大小）**就是每次训练模型时所使用的样本数量。多为2的n次方，如32、64、128等。

较大的 Batch size 会增加训练的内存消耗，但通常会提高计算效率，因为一次性处理的数据量大，能够减少每次更新权重时所需的同步和通信成本。大 Batch size 的另一个优势是它在每次参数更新时，计算的梯度更加平滑，因为它在较大的数据上计算梯度更新，梯度的方差较小。较平滑的梯度更新可以导致模型更快地收敛。当硬件资源充足，并且追求更高的计算效率时，大 Batch size 可能是更好的选择。在稳定的梯度更新情况下，模型的收敛速度也可能更快。不过需要注意的是，过大的 Batch size 可能导致模型过拟合或者泛化性能下降。

较小的 Batch size 则会降低内存需求，但**可能**增加训练时间。当硬件资源有限，或者模型需要更好地泛化时，较小的 Batch size 可能是更合适的选择。尤其在需要模型更灵活地应对不同数据模式时，小 Batch size 可以在训练中引入更多的随机性，帮助模型跳出局部最优解。

### Lteration（迭代）：

一次lteration是指模型使用一个Batch进行一次参数更新。

一次迭代指的是模型使用一个 Batch 进行前向传播和后向传播（即一次模型权重更新的过程）。因此，每次迭代模型只会看到部分训练数据。

### 计算关系：

1 个 Epoch 中包含的迭代次数等于训练数据集的样本数量除以 Batch size。例如，假设训练集有 1000 个样本，Batch size 是 100，那么 1 个 Epoch 中会有 10 次迭代。
$$

\text{Iteration} = \frac{\text{样本数}}{\text{Batch size}}
  
\text{1个 Epoch} = \text{若干次 Iteration}
$$
